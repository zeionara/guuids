# Installing system software

::: tip Disabling terminal sound

To disable annoying sound when doing incorrect actions in the terminal, run the following command:

```sh
sudo modprobe -r pcspkr
```

:::

After reboot, first of all you need to connect to the Internet. There are two options:

::: tabs

== Ethernet

If you are using `Ethernet` cable, no additional configuration is required in most cases, as default configuration supports most hardware by default.

== Wi-Fi

When you need to connect to the Internet via `wifi`, you can use `iw` and `wpa_supplicant` tools. First, make sure that the wireless interface is identified and it is up:

```sh
ip addr
```

If it is down, activate it with command (replace `wlp2s0` with the name of your adapter):

```sh
ip link set wlp2s0 up
```

Then list available networks (replace `wlp2s0` with the name of your adapter):

```sh
iw wlp2s0 scan | grep SSID
```

Generate a minimal configuration file passing the target network `ssid` and `password`:

```sh
wpa_passphrase <ssid> <password> > /etc/wpa_supplicant/wpa_supplicant.conf
```

Update the generated file `/etc/wpa_supplicant/wpa_supplicant.conf` by adding two lines, which would enable non-root users to operate the network adapter and enable support for `wpa_cli` tool:

```sh
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=wheel # [!code ++]
update_config=1 # [!code ++]
network={
	ssid="ssid"
	psk=psk
}
```

Then start `wpa_supplicant` service and add it to the `default` runlevel for enabling network on startup automatically:

```sh
rc-service wpa_supplicant start
rc-update add wpa_supplicant default
```

:::

To make sure you are connected to the Internet, run the following command, which should print your external IP address:

```sh
curl ifconfig.me
```

Then make some cleanup. Delete installation artifacts:

```sh
rm /stage3-*.tar.*
```

Configure the default editor. First, list options:

```sh
eselect editor list
```

And then configure the selected option as the default editor:

```sh
eselect editor set 3
```

Next, configure `/etc/hosts` file by opening it with `vim` and inserting demanded hostnames.

## Create the first user

As the next step, you should create a new user like you did at the beginning of the installation:

```sh
useradd -m -G users,wheel zeio
passwd zeio
export EDITOR=vim
visudo
```

Add the following line to the `sudoers` list:

```sh
root ALL=(ALL:ALL) ALL
zeio ALL=(ALL) NOPASSWD: ALL  # [!code ++]
```

::: warning Disabling root login

After creating a user with admin priviledges, it is recommended to check that the user really became the sudoer, and then disable logging in as `root`:

```sh
passwd -dl root
```

:::

Configure new user's `~/.bash_logout` to disable cleaning command history on logout:

```sh
# /etc/skel/.bash_logout

# This file is sourced when a login shell terminates.

# Clear the screen for security's sake.
clear # [!code --]
```

Then an important step is `ssh` key generation. Use the following command for that:

```sh
ssh-keygen -t ed25519 -C "zeio @ gore" -f $HOME/.ssh/id_ed25519
```

To print the generated key use the following command:

```sh
cat ~/.ssh/id_ed25519.pub
```

::: tip Default user

In the rest of the documentation, it is supposed that the commands are run from a non-`root` sudo user.

:::

## Install auxiliary software

Download and install the following packages:

```sh
sudo emerge --ask sys-process/htop app-misc/neofetch app-portage/gentoolkit
```

Call `neofetch` to print the system info in a beautiful format:

```sh
         -/oyddmdhs+:.                zeio@gore
     -odNMMMMMMMMNNmhy+-`             ---------
   -yNMMMMMMMMMMMNNNmmdhy+-           OS: Gentoo Linux x86_64
 `omMMMMMMMMMMMMNmdmmmmddhhy/`        Host: 80E3 Lenovo G50-45
 omMMMMMMMMMMMNhhyyyohmdddhhhdo`      Kernel: 6.12.63-gentoo-dist
.ydMMMMMMMMMMdhs++so/smdddhhhhdm+`    Uptime: 28 mins
 oyhdmNMMMMMMMNdyooydmddddhhhhyhNd.   Packages: 450 (emerge)
  :oyhhdNNMMMMMMMNNNmmdddhhhhhyymMh   Shell: bash 5.3.9
    .:+sydNMMMMMNNNmmmdddhhhhhhmMmy   Resolution: 1366x768
       /mMMMMMMNNNmmmdddhhhhhmMNhs:   Terminal: /dev/pts/0
    `oNMMMMMMMNNNmmmddddhhdmMNhs+`    CPU: AMD A8-6410 APU with AMD Radeon R5 Graphics (4) @ 2.000GHz
  `sNMMMMMMMMNNNmmmdddddmNMmhs/.      GPU: AMD ATI Radeon R4/R5 Graphics
 /NMMMMMMMMNNNNmmmdddmNMNdso:`        GPU: AMD ATI Radeon HD 8550M / R5 M230
+MMMMMMMNNNNNmmmmdmNMNdso/-           Memory: 173MiB / 14932MiB
yMMNNNNNNNmmmmmNNMmhs+/-`
/hMMNNNNNNNNMNdhs++/-`
`/ohdmmddhys+++/:.`
  `-//////:--.
```

::: tip

To print available package versions use the following command:

```sh
equery list -p htop
```

:::

## Configuring virtualization and acceleration tools

### Installing docker

For being able to run `docker` containers, it is essential to install `docker` and `docker-compose`:

```sh
emerge --ask --verbose app-containers/docker app-containers/docker-cli app-containers/docker-compose
```

After installing `docker` add your user to the `docker` group:

```sh
sudo gpasswd -a zeio docker
```

And restart the shell to apply changes.

::: tip Custom docker root folder

To configure a custom docker root folder create `/etc/docker/daemon.json` with the following content:

```json
{
  "data-root": "/mnt/docker"
}
```

:::

Then configure `docker` to start on boot:

```sh
sudo rc-update add docker default
```

Add enable socket delay for `containerd`:

```sh
echo 'containerd_socket_delay=5' | sudo tee -a /etc/conf.d/containerd
```

And start the service:

```sh
sudo rc-service docker start
```

To run a test container, execute the following command:

```sh
docker run --rm nginx
```

To make sure that `docker-compose` is working, create a basic `docker-compose` file:

```sh
cat > /tmp/docker-compose.yml << EOF
services:
  test:
    image: nginx
EOF
```

Then run the services:

```sh
docker compose -f /tmp/docker-compose.yml up
```

This should emit the standard `nginx` log.

### Installing GPU drivers

Depending on your GPU vendor choose one of the following options.

::: tabs

=== Nvidia

To build the driver from source, enable `kernel-open` USE flag:

```sh
echo 'x11-drivers/nvidia-drivers kernel-open' | sudo tee /etc/portage/package.use/nvidia
```

Then for installing drivers emerge the following package:

```sh
sudo emerge --ask x11-drivers/nvidia-drivers
```

If `emerge` requested `USE changes`, use `etc-update` to accept them and run the command above again. In addition to `nvidia-drivers`, you can install `nvtop` which is a command-line tool for keeping track of GPU consumption (it is analagous to `nvidia-smi`, but is very convenient to track how GPU use is changing over time):

```sh
sudo emerge --ask sys-process/nvtop
```

=== AMD

The driver should have been already installed due to enabled `amdgpu` and `radeonsi` flags in the `VIDEO_CARDS` `USE_EXPAND` variable. To make sure that the driver is loaded and working, run the instruction below:

```sh
lspci -k | grep -A 3 -E "(VGA|3D)"
```

The output should look like this:

```sh{3,4}
00:01.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Mullins [Radeon R4/R5 Graphics] (rev 05)
	Subsystem: Lenovo Device 381a
	Kernel driver in use: radeon
	Kernel modules: radeon, amdgpu
```

Also check the following log:

```sh
sudo dmesg | grep -i radeon
```

Here is my output for reference, which indicates a correct GPU stack (but without `Vulkan` support due to outdated hardware, it is unusual that two GPU devices are detected - `0000:00:01.0` which is a discrete `Mullins` GPU and `0000:01:00.0` which may be a kind of a placeholder):

```sh{1,31,32}
smpboot: CPU0: AMD A8-6410 APU with AMD Radeon R5 Graphics (family: 0x16, model: 0x30, stepping: 0x1)
[drm] radeon kernel modesetting enabled.
radeon 0000:00:01.0: vgaarb: deactivate vga console
radeon 0000:00:01.0: VRAM: 1024M 0x0000000000000000 - 0x000000003FFFFFFF (1024M used)
radeon 0000:00:01.0: GTT: 2048M 0x0000000040000000 - 0x00000000BFFFFFFF
[drm] radeon: 1024M of VRAM memory ready
[drm] radeon: 2048M of GTT memory ready.
Loading firmware: radeon/mullins_pfp.bin
Loading firmware: radeon/mullins_me.bin
Loading firmware: radeon/mullins_ce.bin
Loading firmware: radeon/mullins_mec.bin
Loading firmware: radeon/mullins_rlc.bin
Loading firmware: radeon/mullins_sdma.bin
UBSAN: array-index-out-of-bounds in /var/tmp/portage/sys-kernel/gentoo-kernel-6.12.63/work/linux-6.12/drivers/gpu/drm/radeon/radeon_atombios.c:2720:34
 radeon_atombios_get_power_modes+0x8f6/0x920 [radeon]
 radeon_pm_init+0x142/0x760 [radeon]
 cik_init+0x21d/0x500 [radeon]
 radeon_device_init+0x563/0xba0 [radeon]
 radeon_driver_load_kms+0xa6/0x230 [radeon]
 radeon_drm_ioctl+0x51a/0x7c0 [radeon]
 ? __pfx_init_module+0x10/0x10 [radeon]
UBSAN: array-index-out-of-bounds in /var/tmp/portage/sys-kernel/gentoo-kernel-6.12.63/work/linux-6.12/drivers/gpu/drm/radeon/kv_dpm.c:2508:32
 kv_dpm_init+0x926/0x9c0 [radeon]
 radeon_pm_init+0x53c/0x760 [radeon]
 cik_init+0x21d/0x500 [radeon]
 radeon_device_init+0x563/0xba0 [radeon]
 radeon_driver_load_kms+0xa6/0x230 [radeon]
 radeon_drm_ioctl+0x51a/0x7c0 [radeon]
 ? __pfx_init_module+0x10/0x10 [radeon]
[drm] radeon: dpm initialized
Loading firmware: radeon/bonaire_uvd.bin
Loading firmware: radeon/BONAIRE_vce.bin
radeon 0000:00:01.0: WB enabled
radeon 0000:00:01.0: fence driver on ring 0 use gpu addr 0x0000000040000c00
radeon 0000:00:01.0: fence driver on ring 1 use gpu addr 0x0000000040000c04
radeon 0000:00:01.0: fence driver on ring 2 use gpu addr 0x0000000040000c08
radeon 0000:00:01.0: fence driver on ring 3 use gpu addr 0x0000000040000c0c
radeon 0000:00:01.0: fence driver on ring 4 use gpu addr 0x0000000040000c10
radeon 0000:00:01.0: fence driver on ring 5 use gpu addr 0x0000000000078d30
radeon 0000:00:01.0: fence driver on ring 6 use gpu addr 0x0000000040000c18
radeon 0000:00:01.0: fence driver on ring 7 use gpu addr 0x0000000040000c1c
radeon 0000:00:01.0: radeon: using MSI.
[drm] radeon: irq initialized.
snd_hda_intel 0000:00:01.1: bound 0000:00:01.0 (ops r600_utc [radeon])
[drm] radeon atom DIG backlight initialized
[drm] Radeon Display Connectors
[drm] Initialized radeon 2.50.0 for 0000:00:01.0 on minor 0
fbcon: radeondrmfb (fb0) is primary device
radeon 0000:00:01.0: [drm] fb0: radeondrmfb frame buffer device
radeon 0000:01:00.0: enabling device (0000 -> 0003)
debugfs: File 'radeon_gpu_reset' in directory '/' already present!
debugfs: File 'radeon_fence_info' in directory '/' already present!
radeon 0000:01:00.0: VRAM: 2048M 0x0000000000000000 - 0x000000007FFFFFFF (2048M used)
radeon 0000:01:00.0: GTT: 2048M 0x0000000080000000 - 0x00000000FFFFFFFF
[drm] radeon: 2048M of VRAM memory ready
[drm] radeon: 2048M of GTT memory ready.
debugfs: File 'radeon_vram' in directory '/' already present!
debugfs: File 'radeon_gtt' in directory '/' already present!
debugfs: File 'radeon_vram_mm' in directory '/' already present!
debugfs: File 'radeon_gtt_mm' in directory '/' already present!
Loading firmware: radeon/hainan_pfp.bin
Loading firmware: radeon/hainan_me.bin
Loading firmware: radeon/hainan_ce.bin
Loading firmware: radeon/hainan_rlc.bin
Loading firmware: radeon/hainan_mc.bin
Loading firmware: radeon/hainan_smc.bin
debugfs: File 'radeon_pm_info' in directory '/' already present!
[drm] radeon: dpm initialized
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
radeon 0000:01:00.0: radeon: MSI limited to 32-bit
radeon 0000:01:00.0: radeon: using MSI.
[drm] radeon: irq initialized.
debugfs: File 'radeon_ring_gfx' in directory '/' already present!
debugfs: File 'radeon_ring_cp1' in directory '/' already present!
debugfs: File 'radeon_ring_cp2' in directory '/' already present!
debugfs: File 'radeon_ring_dma1' in directory '/' already present!
debugfs: File 'radeon_ring_dma2' in directory '/' already present!
debugfs: File 'radeon_sa_info' in directory '/' already present!
debugfs: File 'radeon_gem_info' in directory '/' already present!
[drm] Radeon Display Connectors
[drm] Initialized radeon 2.50.0 for 0000:01:00.0 on minor 1
radeon 0000:01:00.0: [drm] No compatible format found
radeon 0000:01:00.0: [drm] Cannot find any crtc or sizes
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
```

To support `vdpau` and `vaapi` enable the corresponding global `USE` flags:

```sh
# These settings were set by the catalyst build script that automatically
# built this stage.
# Please consult /usr/share/portage/config/make.conf.example for a more
# detailed example.

USE="-gtk -gnome dist-kernel dbus alsa wayland bluetooth elogind opengl nvenc lua v4l screencast"  # [!code --]
USE="-gtk -gnome dist-kernel dbus alsa wayland bluetooth elogind opengl nvenc lua v4l screencast vdpau vaapi"  # [!code ++]

COMMON_FLAGS="-march=native -O2 -pipe"
CFLAGS="${COMMON_FLAGS}"
CXXFLAGS="${COMMON_FLAGS}"
FCFLAGS="${COMMON_FLAGS}"
FFLAGS="${COMMON_FLAGS}"

RUSTFLAGS="${RUSTFLAGS} -C target-cpu=native"
MAKEOPTS="-j4 -l5"

# NOTE: This stage was built with the bindist USE flag enabled

# This sets the language of build output to English.
# Please keep this setting intact when reporting bugs.
LC_MESSAGES=C.UTF-8

GENTOO_MIRRORS="http://mirror.mephi.ru/gentoo-distfiles/ \
    ftp://mirror.mephi.ru/gentoo-distfiles/ \
    rsync://mirror.mephi.ru/gentoo-distfiles/ \
    https://mirror.yandex.ru/gentoo-distfiles/ \
    http://mirror.yandex.ru/gentoo-distfiles/ \
    ftp://mirror.yandex.ru/gentoo-distfiles/"
```

Then update dependent packages:

```sh
sudo emerge --ask --changed-use --deep @world
```

Additionally, to enable `Vulkan` (if your GPU is not very old), `OpenCL 2.0`, etc, you *can* install advanced packages. Enable the following `keywords` for the `rocm-opencl-runtime` package:

```sh
echo 'dev-libs/rocm-opencl-runtime ~amd64' | sudo tee /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocr-runtime ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocm-comgr ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocm-device-libs ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-build/rocm-cmake ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-util/hipcc ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/roct-thunk-interface ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
```

Then try to emerge the required packages:

```sh
sudo emerge --ask media-libs/mesa dev-libs/rocm-opencl-runtime
```

Most likely, this will ask you to update file `/etc/portage/package.use/installkernel`

Type:

```sh
> Yes
etc-update
> 1
> 1
> yes
```

Then emerge again:

```sh
sudo emerge --ask media-libs/mesa dev-libs/rocm-opencl-runtime
```

To switch from `radeon` driver to `amdgpu` for testing on older hardware, in `GRUB` menu select the required boot option, then press `e`, then find the line which starts with `linux` and add the following options at the end (here `si` is short for `Sea Islands`, if your GPU belongs to another family, you would likely need to use a different flag):

```sh
radeon.si_support=0 radeon.cik_support=0 amdgpu.si_support=1 amdgpu.cik_support=1
```

Then press `Ctrl+X` or `F10` to boot. After booting to make sure that you are using `amdgpu` driver, run the following command:

```sh
lspci -k | grep -A 3 -E "(VGA|3D)"
```

:::

After installing GPU driver, add your user to `video` group:

```sh
sudo gpasswd -a zeio video
```

Then restart the shell for changes to have an effect. To verify that your setup is now configured for running accelerated AI workloads, you have two paths depending on your GPU vendor.

::: tabs

=== Nvidia

Fist of all, check that `nvidia-smi` is available:

```sh
> nvidia-smi
Sat Jan 24 01:03:14 2026
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   41C    P8               1W /  50W |      3MiB /  4096MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      2766      G   Hyprland                                      1MiB |
+---------------------------------------------------------------------------------------+
```

If `nvidia-smi` is available, then try running `Pytorch` with `CUDA` support. For this, create a test directory:

```sh
cd; mkdir test-cuda; cd test-cuda
```

Create and activate a virtual environment:

```sh
python -m venv .venv
source .venv/bin/activate
```

Then follow instructions on the [pytorch website][pytorch-install] to install `pytroch` with its' dependencies. Most likely, you need to run the following command (this will take some time):

```sh
pip install torch torchvision
```

Then check that `pytorch` recognizes `cuda`-capable device (the command below should output `True`):

```sh
python -c 'import torch; print(torch.cuda.is_available())'
```

To enable `CUDA` support in `docker` containers, emerge `nvidia-container-toolkit`:

```sh
sudo emerge --ask app-containers/nvidia-container-toolkit
```

Configure the installed package:

```sh
sudo nvidia-ctk runtime configure --runtime=docker
```

Restart docker service:

```sh
rc-service docker restart
```

For testing the availability of `CUDA` inside docker containers run a test image:

```sh
docker run --rm --gpus all nvcr.io/nvidia/cuda:13.1.0-devel-ubuntu24.04 nvidia-smi
```

For testing with `docker-compose` create a basic `docker-compose` file:

```sh
cat > $HOME/test-cuda/docker-compose.yml << EOF
services:
  test:
    image: nvcr.io/nvidia/cuda:13.1.0-devel-ubuntu24.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
EOF
```

And run the service:

```sh
docker compose -f $HOME/test-cuda/docker-compose.yml up
```

=== AMD

*To be done...*

:::

[pytorch-install]: https://pytorch.org/get-started/locally
