# Installing command-line tools

::: tip Disable terminal sound

To disable annoying sound when doing incorrect actions in the terminal, run the following command:

```sh
sudo modprobe -r pcspkr
```

:::

After reboot, first of all you need to connect to the Internet. There are two options:

::: tabs

== Ethernet

If you are using `Ethernet` cable, no additional configuration is required in most cases, as default configuration supports most hardware by default.

== Wi-Fi

When you need to connect to the Internet via `wifi`, you can use `iw` and `wpa_supplicant` tools. First, make sure that the wireless interface is identified and it is up:

```sh
ip addr
```

If it is down, activate it with command (replace `wlp2s0` with the name of your adapter):

```sh
ip link set wlp2s0 up
```

Then list available networks (replace `wlp2s0` with the name of your adapter):

```sh
iw wlp2s0 scan | grep SSID
```

Generate a minimal configuration file passing the target network `ssid` and `password`:

```sh
wpa_passphrase <ssid> <password> > /etc/wpa_supplicant/wpa_supplicant.conf
```

Update the generated file `/etc/wpa_supplicant/wpa_supplicant.conf` by adding two lines, which would enable non-root users to operate the network adapter and enable support for `wpa_cli` tool:

```sh
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=wheel # [!code ++]
update_config=1 # [!code ++]
network={
	ssid="ssid"
	psk=psk
}
```

Then start `wpa_supplicant` service and add it to the `default` runlevel for enabling network on startup automatically:

```sh
rc-service wpa_supplicant start
rc-update add wpa_supplicant default
```

:::

To make sure you are connected to the Internet, run the following command, which should print your external IP address:

```sh
curl ifconfig.me
```

Then make some cleanup. Delete installation artifacts:

```sh
rm /stage3-*.tar.*
```

Configure the default editor. First, list options:

```sh
eselect editor list
```

And then configure the selected option as the default editor:

```sh
eselect editor set 3
```

Next, configure `/etc/hosts` file by opening it with `vim` and inserting demanded hostnames.

## Create the first user

As the next step, you should create a new user like you did at the beginning of the installation:

```sh
useradd -m -G users,wheel zeio
passwd zeio
export EDITOR=vim
visudo
```

Add the following line to the `sudoers` list:

```sh
root ALL=(ALL:ALL) ALL
zeio ALL=(ALL) NOPASSWD: ALL  # [!code ++]
```

::: warning Disable root login

After creating a user with admin priviledges, it is recommended to check that the user really became the sudoer, and then disable logging in as `root`:

```sh
passwd -dl root
```

:::

Configure new user's `~/.bash_logout` to disable cleaning command history on logout:

```sh
# /etc/skel/.bash_logout

# This file is sourced when a login shell terminates.

# Clear the screen for security's sake.
clear # [!code --]
```

Then an important step is `ssh` key generation. Use the following command for that:

```sh
ssh-keygen -t ed25519 -C "zeio @ gore" -f $HOME/.ssh/id_ed25519
```

::: tip Copy ssh key to a remote machine

To copy ssh key to a remote machine through the 3rd host, you can download the key to that host like this:

```sh
scp zeio@gore:/home/zeio/.ssh/id_ed25519.pub zeio_gore_ssh.pub
```

And then upload the key to target machine:

```sh
ssh-copy-id -fi zeio_gore_ssh.pub zeio@target
```

:::

To print the generated key use the following command:

```sh
cat ~/.ssh/id_ed25519.pub
```

::: tip Default user

In the rest of the documentation, it is supposed that the commands are run from a non-`root` sudo user.

:::

## Install auxiliary software

Download and install the following packages:

```sh
sudo emerge --ask sys-process/htop app-misc/neofetch app-portage/gentoolkit
```

Call `neofetch` to print the system info in a beautiful format:

```sh
         -/oyddmdhs+:.                zeio@gore
     -odNMMMMMMMMNNmhy+-`             ---------
   -yNMMMMMMMMMMMNNNmmdhy+-           OS: Gentoo Linux x86_64
 `omMMMMMMMMMMMMNmdmmmmddhhy/`        Host: 80E3 Lenovo G50-45
 omMMMMMMMMMMMNhhyyyohmdddhhhdo`      Kernel: 6.12.63-gentoo-dist
.ydMMMMMMMMMMdhs++so/smdddhhhhdm+`    Uptime: 28 mins
 oyhdmNMMMMMMMNdyooydmddddhhhhyhNd.   Packages: 450 (emerge)
  :oyhhdNNMMMMMMMNNNmmdddhhhhhyymMh   Shell: bash 5.3.9
    .:+sydNMMMMMNNNmmmdddhhhhhhmMmy   Resolution: 1366x768
       /mMMMMMMNNNmmmdddhhhhhmMNhs:   Terminal: /dev/pts/0
    `oNMMMMMMMNNNmmmddddhhdmMNhs+`    CPU: AMD A8-6410 APU with AMD Radeon R5 Graphics (4) @ 2.000GHz
  `sNMMMMMMMMNNNmmmdddddmNMmhs/.      GPU: AMD ATI Radeon R4/R5 Graphics
 /NMMMMMMMMNNNNmmmdddmNMNdso:`        GPU: AMD ATI Radeon HD 8550M / R5 M230
+MMMMMMMNNNNNmmmmdmNMNdso/-           Memory: 173MiB / 14932MiB
yMMNNNNNNNmmmmmNNMmhs+/-`
/hMMNNNNNNNNMNdhs++/-`
`/ohdmmddhys+++/:.`
  `-//////:--.
```

::: tip

To print available package versions use the following command:

```sh
equery list -p htop
```

:::

## Configure virtualization and acceleration tools

### Install docker

For being able to run `docker` containers, it is essential to install `docker` and `docker-compose`:

```sh
emerge --ask --verbose app-containers/docker app-containers/docker-cli app-containers/docker-compose
```

After installing `docker` add your user to the `docker` group:

```sh
sudo gpasswd -a zeio docker
```

And restart the shell to apply changes.

::: tip Custom docker root folder

To configure a custom docker root folder create `/etc/docker/daemon.json` with the following content:

```json
{
  "data-root": "/mnt/docker"
}
```

:::

Then configure `docker` to start on boot:

```sh
sudo rc-update add docker default
```

::: tip Enable containerd socket delay

If docker is not starting automatically after reboot, try enabling socket delay for `containerd`:

```sh
echo 'containerd_socket_delay=5' | sudo tee -a /etc/conf.d/containerd
```

:::

And start the service:

```sh
sudo rc-service docker start
```

To run a test container, execute the following command:

```sh
docker run --rm nginx
```

To make sure that `docker-compose` is working, create a basic `docker-compose` file:

```sh
cat > /tmp/docker-compose.yml << EOF
services:
  test:
    image: nginx
EOF
```

Then run the services:

```sh
docker compose -f /tmp/docker-compose.yml up
```

This should emit the standard `nginx` log.

### Install GPU drivers

Depending on your GPU vendor choose one of the following options.

::: tabs

=== Nvidia

To build the driver from source, enable `kernel-open` USE flag:

```sh
echo 'x11-drivers/nvidia-drivers kernel-open' | sudo tee /etc/portage/package.use/nvidia
```

Then for installing drivers emerge the following package:

```sh
sudo emerge --ask x11-drivers/nvidia-drivers
```

If `emerge` requested `USE changes`, use `etc-update` to accept them and run the command above again. In addition to `nvidia-drivers`, you can install `nvtop` which is a command-line tool for keeping track of GPU consumption (it is analagous to `nvidia-smi`, but is very convenient to track how GPU use is changing over time):

```sh
sudo emerge --ask sys-process/nvtop
```

=== AMD

The driver should have been already installed due to enabled `amdgpu` and `radeonsi` flags in the `VIDEO_CARDS` `USE_EXPAND` variable. To make sure that the driver is loaded and working, run the instruction below:

```sh
lspci -k | grep -A 3 -E "(VGA|3D)"
```

The output should look like this:

```sh{3,4}
00:01.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Mullins [Radeon R4/R5 Graphics] (rev 05)
	Subsystem: Lenovo Device 381a
	Kernel driver in use: radeon
	Kernel modules: radeon, amdgpu
```

Also check the following log:

```sh
sudo dmesg | grep -i radeon
```

Here is my output for reference, which indicates a correct GPU stack (but without `Vulkan` support due to outdated hardware, it is unusual that two GPU devices are detected - `0000:00:01.0` which is a discrete `Mullins` GPU and `0000:01:00.0` which may be a kind of a placeholder):

```sh{1,31,32}
smpboot: CPU0: AMD A8-6410 APU with AMD Radeon R5 Graphics (family: 0x16, model: 0x30, stepping: 0x1)
[drm] radeon kernel modesetting enabled.
radeon 0000:00:01.0: vgaarb: deactivate vga console
radeon 0000:00:01.0: VRAM: 1024M 0x0000000000000000 - 0x000000003FFFFFFF (1024M used)
radeon 0000:00:01.0: GTT: 2048M 0x0000000040000000 - 0x00000000BFFFFFFF
[drm] radeon: 1024M of VRAM memory ready
[drm] radeon: 2048M of GTT memory ready.
Loading firmware: radeon/mullins_pfp.bin
Loading firmware: radeon/mullins_me.bin
Loading firmware: radeon/mullins_ce.bin
Loading firmware: radeon/mullins_mec.bin
Loading firmware: radeon/mullins_rlc.bin
Loading firmware: radeon/mullins_sdma.bin
UBSAN: array-index-out-of-bounds in /var/tmp/portage/sys-kernel/gentoo-kernel-6.12.63/work/linux-6.12/drivers/gpu/drm/radeon/radeon_atombios.c:2720:34
 radeon_atombios_get_power_modes+0x8f6/0x920 [radeon]
 radeon_pm_init+0x142/0x760 [radeon]
 cik_init+0x21d/0x500 [radeon]
 radeon_device_init+0x563/0xba0 [radeon]
 radeon_driver_load_kms+0xa6/0x230 [radeon]
 radeon_drm_ioctl+0x51a/0x7c0 [radeon]
 ? __pfx_init_module+0x10/0x10 [radeon]
UBSAN: array-index-out-of-bounds in /var/tmp/portage/sys-kernel/gentoo-kernel-6.12.63/work/linux-6.12/drivers/gpu/drm/radeon/kv_dpm.c:2508:32
 kv_dpm_init+0x926/0x9c0 [radeon]
 radeon_pm_init+0x53c/0x760 [radeon]
 cik_init+0x21d/0x500 [radeon]
 radeon_device_init+0x563/0xba0 [radeon]
 radeon_driver_load_kms+0xa6/0x230 [radeon]
 radeon_drm_ioctl+0x51a/0x7c0 [radeon]
 ? __pfx_init_module+0x10/0x10 [radeon]
[drm] radeon: dpm initialized
Loading firmware: radeon/bonaire_uvd.bin
Loading firmware: radeon/BONAIRE_vce.bin
radeon 0000:00:01.0: WB enabled
radeon 0000:00:01.0: fence driver on ring 0 use gpu addr 0x0000000040000c00
radeon 0000:00:01.0: fence driver on ring 1 use gpu addr 0x0000000040000c04
radeon 0000:00:01.0: fence driver on ring 2 use gpu addr 0x0000000040000c08
radeon 0000:00:01.0: fence driver on ring 3 use gpu addr 0x0000000040000c0c
radeon 0000:00:01.0: fence driver on ring 4 use gpu addr 0x0000000040000c10
radeon 0000:00:01.0: fence driver on ring 5 use gpu addr 0x0000000000078d30
radeon 0000:00:01.0: fence driver on ring 6 use gpu addr 0x0000000040000c18
radeon 0000:00:01.0: fence driver on ring 7 use gpu addr 0x0000000040000c1c
radeon 0000:00:01.0: radeon: using MSI.
[drm] radeon: irq initialized.
snd_hda_intel 0000:00:01.1: bound 0000:00:01.0 (ops r600_utc [radeon])
[drm] radeon atom DIG backlight initialized
[drm] Radeon Display Connectors
[drm] Initialized radeon 2.50.0 for 0000:00:01.0 on minor 0
fbcon: radeondrmfb (fb0) is primary device
radeon 0000:00:01.0: [drm] fb0: radeondrmfb frame buffer device
radeon 0000:01:00.0: enabling device (0000 -> 0003)
debugfs: File 'radeon_gpu_reset' in directory '/' already present!
debugfs: File 'radeon_fence_info' in directory '/' already present!
radeon 0000:01:00.0: VRAM: 2048M 0x0000000000000000 - 0x000000007FFFFFFF (2048M used)
radeon 0000:01:00.0: GTT: 2048M 0x0000000080000000 - 0x00000000FFFFFFFF
[drm] radeon: 2048M of VRAM memory ready
[drm] radeon: 2048M of GTT memory ready.
debugfs: File 'radeon_vram' in directory '/' already present!
debugfs: File 'radeon_gtt' in directory '/' already present!
debugfs: File 'radeon_vram_mm' in directory '/' already present!
debugfs: File 'radeon_gtt_mm' in directory '/' already present!
Loading firmware: radeon/hainan_pfp.bin
Loading firmware: radeon/hainan_me.bin
Loading firmware: radeon/hainan_ce.bin
Loading firmware: radeon/hainan_rlc.bin
Loading firmware: radeon/hainan_mc.bin
Loading firmware: radeon/hainan_smc.bin
debugfs: File 'radeon_pm_info' in directory '/' already present!
[drm] radeon: dpm initialized
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
radeon 0000:01:00.0: radeon: MSI limited to 32-bit
radeon 0000:01:00.0: radeon: using MSI.
[drm] radeon: irq initialized.
debugfs: File 'radeon_ring_gfx' in directory '/' already present!
debugfs: File 'radeon_ring_cp1' in directory '/' already present!
debugfs: File 'radeon_ring_cp2' in directory '/' already present!
debugfs: File 'radeon_ring_dma1' in directory '/' already present!
debugfs: File 'radeon_ring_dma2' in directory '/' already present!
debugfs: File 'radeon_sa_info' in directory '/' already present!
debugfs: File 'radeon_gem_info' in directory '/' already present!
[drm] Radeon Display Connectors
[drm] Initialized radeon 2.50.0 for 0000:01:00.0 on minor 1
radeon 0000:01:00.0: [drm] No compatible format found
radeon 0000:01:00.0: [drm] Cannot find any crtc or sizes
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
radeon 0000:01:00.0: WB enabled
radeon 0000:01:00.0: fence driver on ring 0 use gpu addr 0x0000000080000c00
radeon 0000:01:00.0: fence driver on ring 1 use gpu addr 0x0000000080000c04
radeon 0000:01:00.0: fence driver on ring 2 use gpu addr 0x0000000080000c08
radeon 0000:01:00.0: fence driver on ring 3 use gpu addr 0x0000000080000c0c
radeon 0000:01:00.0: fence driver on ring 4 use gpu addr 0x0000000080000c10
```

To support `vdpau` and `vaapi` enable the corresponding global `USE` flags:

```sh
# These settings were set by the catalyst build script that automatically
# built this stage.
# Please consult /usr/share/portage/config/make.conf.example for a more
# detailed example.

USE="-gtk -gnome dist-kernel dbus alsa wayland bluetooth elogind opengl nvenc lua v4l screencast"  # [!code --]
USE="-gtk -gnome dist-kernel dbus alsa wayland bluetooth elogind opengl nvenc lua v4l screencast vdpau vaapi"  # [!code ++]

COMMON_FLAGS="-march=native -O2 -pipe"
CFLAGS="${COMMON_FLAGS}"
CXXFLAGS="${COMMON_FLAGS}"
FCFLAGS="${COMMON_FLAGS}"
FFLAGS="${COMMON_FLAGS}"

RUSTFLAGS="${RUSTFLAGS} -C target-cpu=native"
MAKEOPTS="-j4 -l5"

# NOTE: This stage was built with the bindist USE flag enabled

# This sets the language of build output to English.
# Please keep this setting intact when reporting bugs.
LC_MESSAGES=C.UTF-8

GENTOO_MIRRORS="http://mirror.mephi.ru/gentoo-distfiles/ \
    ftp://mirror.mephi.ru/gentoo-distfiles/ \
    rsync://mirror.mephi.ru/gentoo-distfiles/ \
    https://mirror.yandex.ru/gentoo-distfiles/ \
    http://mirror.yandex.ru/gentoo-distfiles/ \
    ftp://mirror.yandex.ru/gentoo-distfiles/"
```

Then update dependent packages:

```sh
sudo emerge --ask --changed-use --deep @world
```

Additionally, to enable `Vulkan` (if your GPU is not very old), `OpenCL 2.0`, etc, you *can* install advanced packages. Enable the following `keywords` for the `rocm-opencl-runtime` package:

```sh
echo 'dev-libs/rocm-opencl-runtime ~amd64' | sudo tee /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocr-runtime ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocm-comgr ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/rocm-device-libs ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-build/rocm-cmake ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-util/hipcc ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
echo 'dev-libs/roct-thunk-interface ~amd64' | sudo tee -a /etc/portage/package.accept_keywords/rocm-opencl-runtime
```

Then try to emerge the required packages:

```sh
sudo emerge --ask media-libs/mesa dev-libs/rocm-opencl-runtime
```

Most likely, this will ask you to update file `/etc/portage/package.use/installkernel`

Type:

```sh
> Yes
etc-update
> 1
> 1
> yes
```

Then emerge again:

```sh
sudo emerge --ask media-libs/mesa dev-libs/rocm-opencl-runtime
```

To switch from `radeon` driver to `amdgpu` for testing on older hardware, in `GRUB` menu select the required boot option, then press `e`, then find the line which starts with `linux` and add the following options at the end (here `si` is short for `Sea Islands`, if your GPU belongs to another family, you would likely need to use a different flag):

```sh
radeon.si_support=0 radeon.cik_support=0 amdgpu.si_support=1 amdgpu.cik_support=1
```

Then press `Ctrl+X` or `F10` to boot. After booting to make sure that you are using `amdgpu` driver, run the following command:

```sh
lspci -k | grep -A 3 -E "(VGA|3D)"
```

:::

After installing GPU driver, add your user to `video` group:

```sh
sudo gpasswd -a zeio video
```

Then restart the shell for changes to have an effect. To verify that your setup is now configured for running accelerated AI workloads, you have two paths depending on your GPU vendor.

::: tabs

=== Nvidia

Fist of all, check that `nvidia-smi` is available:

```sh
> nvidia-smi
Sat Jan 24 01:03:14 2026
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   41C    P8               1W /  50W |      3MiB /  4096MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      2766      G   Hyprland                                      1MiB |
+---------------------------------------------------------------------------------------+
```

If `nvidia-smi` is available, then try running `Pytorch` with `CUDA` support. For this, create a test directory:

```sh
cd; mkdir test-cuda; cd test-cuda
```

Create and activate a virtual environment:

```sh
python -m venv .venv
source .venv/bin/activate
```

Then follow instructions on the [pytorch website][pytorch-install] to install `pytroch` with its' dependencies. Most likely, you need to run the following command (this will take some time):

```sh
pip install torch torchvision
```

Then check that `pytorch` recognizes `cuda`-capable device (the command below should output `True`):

```sh
python -c 'import torch; print(torch.cuda.is_available())'
```

To enable `CUDA` support in `docker` containers, emerge `nvidia-container-toolkit`:

```sh
sudo emerge --ask app-containers/nvidia-container-toolkit
```

Configure the installed package:

```sh
sudo nvidia-ctk runtime configure --runtime=docker
```

Restart docker service:

```sh
rc-service docker restart
```

For testing the availability of `CUDA` inside docker containers run a test image:

```sh
docker run --rm --gpus all nvcr.io/nvidia/cuda:13.1.0-devel-ubuntu24.04 nvidia-smi
```

For testing with `docker-compose` create a basic `docker-compose` file:

```sh
cat > $HOME/test-cuda/docker-compose.yml << EOF
services:
  test:
    image: nvcr.io/nvidia/cuda:13.1.0-devel-ubuntu24.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                - gpu
EOF
```

And run the service:

```sh
docker compose -f $HOME/test-cuda/docker-compose.yml up
```

=== AMD

*To be done...*

:::

## Install zsh

To install `zsh` emerge the required package:

```sh
sudo emerge --ask app-shells/zsh
```

Make `zsh` the default shell:

```sh
chsh -s /usr/bin/zsh
```

Then sign out for the changes to have an effect. When you sign back in, there is `zsh` configuration window. Feel free to select `0` to exit configuration. Add your public `ssh` key to github and clone my fork of `ohmyzsh` with some presets (you will would need to either delete or create an own fork of `/custom/bashrc`, because it is a private repo):

```sh
export REPO=zeionara/ohmyzsh
sh -c "$(curl -fsSL https://raw.githubusercontent.com/zeionara/ohmyzsh/master/tools/install.sh)"
cd $HOME/.ohmyzsh
git submodule update --init
echo "source $HOME/.oh-my-zsh/.zshrc" | tee $HOME/.zshrc
```

Then sign out and sign in back for the changes to have an effect, and install a custom theme if you'd like:

```sh
fast-theme $HOME/.oh-my-zsh/custom/shell/snuffari.ini
```

## Generate gpg2 signature

Create `gpg2` signature and enable commit signing by default:

```sh
gpgeng
```

Install this signature to `github` and `gitlab`.

::: tip Change git pager

To set git pager to `less` use the following command:

```sh
git config --global --replace-all core.pager "less -F -X"
```
:::

## Configure vim

Install plugin manager by running the following command:

```sh
curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim
```

Create `$HOME/.vimrc` using the following script:

```sh
cat <<EOF > $HOME/.vimrc
set number
set autochdir
set autoindent

set tabstop=2
set shiftwidth=2
set expandtab

set cursorline
set colorcolumn=200
set so=999

set spell
set spelllang=en_us
set t_Co=256

call plug#begin()
Plug 'vim-airline/vim-airline'
Plug 'vim-airline/vim-airline-themes'
Plug 'prabirshrestha/vim-lsp'
Plug 'dense-analysis/ale'
Plug 'airblade/vim-gitgutter'
Plug 'ghifarit53/tokyonight-vim'
call plug#end()

set termguicolors
let g:tokyonight_style = 'night' " available: night, storm
colorscheme tokyonight

let g:airline_theme='night_owl'
let g:airline#extensions#branch#enabled=1
let g:airline_powerline_fonts = 1

nmap <C-K> :ALENext<cr>
EOF
```

::: tip Disabling ale

To disable `ale` add the following line to `.vimrc`:

```sh
let g:ale_enabled = 0
```

:::

Start `vim`, ignore errors for now and run `:PlugInstall`.

### Install language servers

#### pylsp

Add the following block to `.vimrc`:

```sh
cat <<EOF >> $HOME/.vimrc

" pylsp

let g:line_length = 200

if executable('pylsp')
  au User lsp_setup call lsp#register_server({
      \ 'name': 'pylsp',
      \ 'cmd': {server_info->['pylsp']},
      \ 'allowlist': ['python'],
      \ 'workspace_config': {
      \   'pylsp': {
      \       'configurationSources': ['pycodestyle', 'flake8'],
      \       'plugins': {
      \           'pylint': {'enabled': v:true},
      \           'mccabre': {'enabled': v:true},
      \           'jedi_completion': {'enabled': v:true},
      \           'jedi_hover': {'enabled': v:true},
      \           'jedi_references': {'enabled': v:true},
      \           'jedi_signature_help': {'enabled': v:true},
      \           'jedi_symbols': {'enabled': v:true},
      \           'rope_completion': {'enabled': v:true},
      \           'ruff': {
      \                'enabled': v:true,
      \                'formatEnabled': v:true,
      \                'lineLength': g:line_length,
      \                'preview': v:true
      \           },
      \           'flake8': {
      \               'enabled': v:true,
      \               'maxLineLength': g:line_length
      \           },
      \           'pyflakes': {'enabled': v:true},
      \           'pydocstyle': {'enabled': v:true},
      \           'pycodestyle': {
      \               'enabled': v:true,
      \               'maxLineLength': g:line_length,
      \               'ignore': [
      \                   'E251',
      \                   'W504',
      \               ]
      \           },
      \           'black': {'enable': v:true},
      \           'autopep8': {'enable': v:true},
      \       },
      \   },
      \ },
      \})
endif

if exists('+tagfunc')
    setlocal tagfunc=lsp#tagfunc
endif

function! s:on_lsp_buffer_enabled() abort
    setlocal omnifunc=lsp#complete
    setlocal signcolumn=yes
    if exists('+tagfunc') | setlocal tagfunc=lsp#tagfunc | endif
    nmap <buffer> gd <plug>(lsp-definition)
    nmap <buffer> gs <plug>(lsp-document-symbol-search)
    nmap <buffer> gS <plug>(lsp-workspace-symbol-search)
    nmap <buffer> gr <plug>(lsp-references)
    nmap <buffer> gi <plug>(lsp-implementation)
    nmap <buffer> gt <plug>(lsp-type-definition)
    nmap <buffer> <leader>rn <plug>(lsp-rename)
    nmap <buffer> [g <plug>(lsp-previous-diagnostic)
    nmap <buffer> ]g <plug>(lsp-next-diagnostic)
    nmap <buffer> K <plug>(lsp-hover)
    nnoremap <buffer> <expr><c-f> lsp#scroll(+4)
    nnoremap <buffer> <expr><c-d> lsp#scroll(-4)

    let g:lsp_format_sync_timeout = 1000
    autocmd! BufWritePre *.rs,*.go call execute('LspDocumentFormatSync')

    " refer to doc to add more commands
endfunction

augroup lsp_install
    au!
    " call s:on_lsp_buffer_enabled only for languages that has the server registered.
    autocmd User lsp_buffer_enabled call s:on_lsp_buffer_enabled()
augroup END

inoremap <C-n> <C-x><C-o>
inoremap <expr> <CR> pumvisible() ? "\<C-y>\<Esc>" : "\<CR>"
EOF
```

To test `pylsp` language server create a dedicated project and environment:

```sh
cd; mkdir test-pylsp; cd test-pylsp
python -m venv .venv
source .venv/bin/activate
pip install "python-lsp-server[all]"
```

Then create a test file:

```sh
cat <<EOF > main.py
import torch


if __name__ == "__main__":
  print(torch.cuda.is_available())


print(foo)
EOF
```

Open the file with `vim`:

```sh
vim main.py
```

::: tip Debugging vim-lsp

To debug `vim-lsp` enable verbose logging and redirect log to a file:

```sh
let g:lsp_log_verbose = 1
let g:lsp_log_file = expand('~/vim-lsp.log')
```
:::

## Configure tmux

Install tmux by emerging the following package:

```sh
sudo emerge --ask app-misc/tmux
```

Download tmux config by cloning the appropriate repo:

::: tabs

=== ssh

```sh
git clone git@github.com:zeionara/tmux-config.git $HOME/tmux-config
```

=== https

```sh
https://github.com/zeionara/tmux-config.git $HOME/tmux-config
```

:::

Install plugins:

```sh
$HOME/tmux-config/install.sh
```

## Configure openvpn

Install the required packages:

```sh

```

Copy configuration files to `/etc/openvpn` like this:

```sh
sudo cp foo-bar.conf /etc/openvpn
sudo chown root:root /etc/openvpn/foo-bar.conf
sudo chmod 400 /etc/openvpn/foo-bar.conf
```

Create symbolic link with the configuration filename:

```sh
sudo ln -s /etc/init.d/openvpn /etc/init.d/openvpn.foo-bar
```

Add the service to the `default` run level:

```sh
sudo rc-update add openvpn.foo-bar default
```

And start the service in current session:

```sh
sudo rc-service openvpn.foo-bar start
```

Reboot to make sure that openvpn is started automatically.

## Configure mihomo proxy

Enable the portage overlay:

```sh
sudo eselect repository enable gentoo-zh
```

Pull the latest registry changes:

```sh
sudo emaint sync -r gentoo-zh
```

Enable `amd64` keyword for `mihomo` package:

```sh
echo 'net-proxy/mihomo ~amd64' | sudo tee /etc/portage/package.accept_keywords/mihomo
```

Install `mihomo` package:

```sh
sudo emerge --ask net-proxy/mihomo
```

Then install `GeoIP` and `GeoSite` files:

```sh
mkdir /home/$USER/.config/mihomo
wget -O /home/$USER/.config/mihomo/GeoIP.dat "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geoip.dat"
wget -O /home/$USER/.config/mihomo/GeoSite.dat "https://github.com/Loyalsoldier/v2ray-rules-dat/releases/latest/download/geosite.dat"
```

And update capabilities of `mihomo` binary:

```sh
sudo setcap cap_net_admin+ep $(which mihomo)
```

Pull the configuration file (env variable `MIHOMO_CONFIG_URL` must contain the correct value):

```sh
mupd
```

Check your current ip address and start mihomo:

```sh
curl ifconfig.me
mup
```

Check you ip address and stop mihomo:

```sh
curl ifconfig.me
mdn
```

Check your ip address again, it should be set back to its original value:

```sh
curl ifconfig.me
```

[pytorch-install]: https://pytorch.org/get-started/locally
